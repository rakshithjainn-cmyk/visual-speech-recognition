from nbformat import v4 as nbf

# Full notebook cells
notebook_cells = []

code = r"""
# ==============================================================
# Visual Speech Recognition (Lip Reading) - CNN-LSTM DEMO
# Google Colab Ready - Self-contained with Dummy Dataset
# ==============================================================

!pip install tensorflow opencv-python scikit-learn matplotlib tqdm --quiet

import os
import numpy as np
import cv2
import random
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

# ==============================================================
# 1. Create a Dummy Dataset
# ==============================================================

def create_dummy_dataset(root_dir="dummy_dataset", n_classes=3, n_samples_per_class=20,
                         seq_len=10, img_size=64):
    np.random.seed(42)
    os.makedirs(root_dir, exist_ok=True)
    splits = ["train", "val", "test"]
    for split in splits:
        for c in range(n_classes):
            class_dir = os.path.join(root_dir, split, f"class_{c}")
            os.makedirs(class_dir, exist_ok=True)
            for i in range(n_samples_per_class if split=="train" else n_samples_per_class//2):
                sample_dir = os.path.join(class_dir, f"sample_{i}")
                os.makedirs(sample_dir, exist_ok=True)
                for f in range(seq_len):
                    img = np.zeros((img_size,img_size,3), dtype=np.uint8)
                    img[:,:,c] = np.random.randint(100,255)  # Add color channel variation
                    cv2.imwrite(os.path.join(sample_dir, f"{f:04d}.jpg"), img)

create_dummy_dataset()
print("Dummy dataset created!")

# ==============================================================
# 2. Data Loader
# ==============================================================

def load_sequence(folder, seq_len=10, img_size=64):
    files = sorted([f for f in os.listdir(folder) if f.endswith(".jpg")])
    frames = []
    for f in files[:seq_len]:
        img = cv2.imread(os.path.join(folder, f))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (img_size, img_size))
        frames.append(img.astype(np.float32)/255.0)
    while len(frames) < seq_len:
        frames.append(frames[-1])
    return np.array(frames)

def scan_dataset(root, seq_len=10, img_size=64):
    data, labels = [], []
    classes = sorted(os.listdir(root))
    for cls in classes:
        class_path = os.path.join(root, cls)
        for sample in os.listdir(class_path):
            sample_path = os.path.join(class_path, sample)
            if os.path.isdir(sample_path):
                data.append(load_sequence(sample_path, seq_len, img_size))
                labels.append(cls)
    return np.array(data), np.array(labels), classes

seq_len, img_size = 10, 64
X_train, y_train, classes = scan_dataset("dummy_dataset/train", seq_len, img_size)
X_val, y_val, _ = scan_dataset("dummy_dataset/val", seq_len, img_size)
X_test, y_test, _ = scan_dataset("dummy_dataset/test", seq_len, img_size)

print(f"Train shape: {X_train.shape}, Labels: {len(classes)}")

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc = le.transform(y_val)
y_test_enc = le.transform(y_test)

n_classes = len(classes)
y_train_cat = tf.keras.utils.to_categorical(y_train_enc, n_classes)
y_val_cat = tf.keras.utils.to_categorical(y_val_enc, n_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test_enc, n_classes)

# ==============================================================
# 3. CNN-LSTM Model
# ==============================================================

def build_cnn_lstm(seq_len=10, img_size=64, channels=3, n_classes=3):
    inp = layers.Input(shape=(seq_len, img_size, img_size, channels))
    cnn = models.Sequential([
        layers.Conv2D(32,3,activation='relu',padding='same'),
        layers.MaxPooling2D(),
        layers.Conv2D(64,3,activation='relu',padding='same'),
        layers.MaxPooling2D(),
        layers.Conv2D(128,3,activation='relu',padding='same'),
        layers.GlobalAveragePooling2D()
    ])
    x = layers.TimeDistributed(cnn)(inp)
    x = layers.Bidirectional(layers.LSTM(128))(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    model = models.Model(inp, out)
    return model

model = build_cnn_lstm(seq_len, img_size, 3, n_classes)
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

# ==============================================================
# 4. Training
# ==============================================================

callbacks = [EarlyStopping(patience=5, restore_best_weights=True)]
history = model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=20, batch_size=4, callbacks=callbacks
)

# ==============================================================
# 5. Evaluation
# ==============================================================

loss, acc = model.evaluate(X_test, y_test_cat)
print(f"Test Accuracy: {acc*100:.2f}%")

y_pred = np.argmax(model.predict(X_test), axis=1)
print(classification_report(y_test_enc, y_pred, target_names=classes))

# ==============================================================
# 6. Visualize Training
# ==============================================================

import matplotlib.pyplot as plt
plt.figure(figsize=(8,4))
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title("Training History")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
"""

notebook = nbf.new_notebook(cells=[nbf.new_code_cell(code)])

# Save notebook
output_ipynb = "/mnt/data/Visual_Speech_Recognition_Demo.ipynb"
with open(output_ipynb, "w") as f:
    import nbformat
    nbformat.write(notebook, f)

output_ipynb
